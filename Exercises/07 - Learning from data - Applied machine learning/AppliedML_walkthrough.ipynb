{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94956225",
   "metadata": {},
   "source": [
    "# AppliedML.ipynb Walkthrough (Exam-Focused)\n",
    "\n",
    "This is a guided walkthrough of `AppliedML.ipynb`, using `AppliedML_solutions.ipynb` as a reference. It explains *what to do* in each part and *why*, with minimal “magic”.\n",
    "\n",
    "## What you’re building\n",
    "\n",
    "- **Goal:** predict whether an animal’s `outcome_type` is **Adoption** (`adopted = 1`) or **not** (`adopted = 0`).\n",
    "- **Model:** logistic regression (binary classifier that outputs probabilities).\n",
    "- **Main pipeline:** load → clean → label → train/test split → one-hot encode categoricals → standardize → fit model → evaluate with different thresholds → interpret coefficients.\n",
    "\n",
    "## Libraries used (what they’re for)\n",
    "\n",
    "- `pandas` (`pd`): load CSV, manipulate tabular data (`DataFrame`), one-hot encoding via `pd.get_dummies`.\n",
    "- `numpy` (`np`): numeric operations, random splitting, thresholding, counting TP/FP/FN/TN.\n",
    "- `sklearn.linear_model.LogisticRegression`: logistic regression classifier with `.fit()` and `.predict_proba()`.\n",
    "- `matplotlib.pyplot` (`plt`) + `seaborn` (`sn`): plotting (curves, confusion matrix heatmap, coefficient bars).\n",
    "\n",
    "---\n",
    "\n",
    "## Part A — Load, encode, split, standardize (only `numpy` + `pandas`)\n",
    "\n",
    "### A1) Load + drop missing rows\n",
    "\n",
    "\n",
    "You’re allowed to drop missing entries:\n",
    "\n",
    "```python\n",
    "print('rows before:', len(original_data))\n",
    "original_data = original_data.dropna().reset_index(drop=True)\n",
    "print('rows after: ', len(original_data))\n",
    "```\n",
    "\n",
    "### A2) Create label and features table\n",
    "\n",
    "Turn `outcome_type` into a binary label and drop the original column:\n",
    "\n",
    "```python\n",
    "data_features = original_data.copy()\n",
    "data_features['adopted'] = (data_features['outcome_type'] == 'Adoption').astype(int)\n",
    "data_features = data_features.drop(columns=['outcome_type'])\n",
    "```\n",
    "\n",
    "### A3) Split into train/test (80/20)\n",
    "\n",
    "A simple split without external libraries:\n",
    "\n",
    "```python\n",
    "def split_set(df, ratio=0.8, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    mask = rng.random(len(df)) < ratio\n",
    "    return df[mask].reset_index(drop=True), df[~mask].reset_index(drop=True)\n",
    "\n",
    "train, test = split_set(data_features, ratio=0.8, seed=0)\n",
    "```\n",
    "\n",
    "If you want to match the solutions notebook *exactly*, remove the `seed` and use `np.random.rand(...)` instead.\n",
    "\n",
    "### A4) Dummy-variable encoding for categoricals\n",
    "\n",
    "Pick the categorical columns and one-hot encode:\n",
    "\n",
    "```python\n",
    "categorical_columns = [\n",
    "    'sex_upon_outcome', 'animal_type', 'intake_condition',\n",
    "    'intake_type', 'sex_upon_intake'\n",
    "]\n",
    "\n",
    "train_enc = pd.get_dummies(train, columns=categorical_columns)\n",
    "```\n",
    "\n",
    "Important: the test set must have the **same columns in the same order** as the training set:\n",
    "\n",
    "```python\n",
    "test_enc = pd.get_dummies(test, columns=categorical_columns)\n",
    "test_enc = test_enc.reindex(columns=train_enc.columns, fill_value=0)\n",
    "```\n",
    "\n",
    "### A5) Separate labels from features\n",
    "\n",
    "```python\n",
    "train_y = train_enc['adopted']\n",
    "train_X = train_enc.drop(columns=['adopted'])\n",
    "\n",
    "test_y = test_enc['adopted']\n",
    "test_X = test_enc.drop(columns=['adopted'])\n",
    "```\n",
    "\n",
    "### A6) Standardize (mean 0, variance 1) using *training stats only*\n",
    "\n",
    "Compute mean/std on training features and apply to both:\n",
    "\n",
    "```python\n",
    "means = train_X.mean()\n",
    "stds = train_X.std().replace(0, 1)  # guard against constant columns\n",
    "\n",
    "train_X_std = (train_X - means) / stds\n",
    "test_X_std  = (test_X  - means) / stds\n",
    "```\n",
    "\n",
    "That completes Part A (preprocessing).\n",
    "\n",
    "---\n",
    "\n",
    "## Part B — Train logistic regression + confusion matrix + manual metrics\n",
    "\n",
    "### B1) Fit logistic regression\n",
    "\n",
    "```python\n",
    "logreg = LogisticRegression(solver='lbfgs', max_iter=10000)\n",
    "logreg.fit(train_X_std, train_y)\n",
    "```\n",
    "\n",
    "### B2) Get predicted probabilities on test\n",
    "\n",
    "`predict_proba` returns an `N x 2` array: `[:,0]` is `P(class=0)`, `[:,1]` is `P(class=1)`:\n",
    "\n",
    "```python\n",
    "proba = logreg.predict_proba(test_X_std)\n",
    "```\n",
    "\n",
    "### B3) Threshold to get binary predictions\n",
    "\n",
    "For threshold `t=0.5`:\n",
    "\n",
    "```python\n",
    "t = 0.5\n",
    "pred_y = (proba[:, 1] > t).astype(int)\n",
    "```\n",
    "\n",
    "### B4) Confusion matrix (TP, FP, FN, TN)\n",
    "\n",
    "Define TP/FP/FN/TN using boolean logic:\n",
    "\n",
    "```python\n",
    "TP = np.sum((pred_y == 1) & (test_y.values == 1))\n",
    "TN = np.sum((pred_y == 0) & (test_y.values == 0))\n",
    "FP = np.sum((pred_y == 1) & (test_y.values == 0))\n",
    "FN = np.sum((pred_y == 0) & (test_y.values == 1))\n",
    "```\n",
    "\n",
    "Many conventions exist; be explicit. A common layout is:\n",
    "\n",
    "```python\n",
    "cm = np.array([[TP, FP],\n",
    "               [FN, TN]])\n",
    "cm\n",
    "```\n",
    "\n",
    "### B5) Manual metrics (positive and negative class)\n",
    "\n",
    "With TP/FP/FN/TN:\n",
    "\n",
    "- **Accuracy:** `(TP + TN) / (TP + TN + FP + FN)`\n",
    "- **Positive precision:** `TP / (TP + FP)`\n",
    "- **Positive recall (TPR):** `TP / (TP + FN)`\n",
    "- **Positive F1:** `2 * prec * rec / (prec + rec)`\n",
    "\n",
    "For the **negative class**, treat “negative” as the class of interest:\n",
    "\n",
    "- **Negative precision:** `TN / (TN + FN)`\n",
    "- **Negative recall (TNR / specificity):** `TN / (TN + FP)`\n",
    "- **Negative F1:** computed the same way from negative precision/recall\n",
    "\n",
    "---\n",
    "\n",
    "## Part C — Sweep threshold and plot metrics vs threshold\n",
    "\n",
    "### C1) Loop over thresholds and compute metrics\n",
    "\n",
    "```python\n",
    "thresholds = np.linspace(0, 1, 100)\n",
    "rows = []\n",
    "\n",
    "for t in thresholds:\n",
    "    pred_y = (proba[:, 1] > t).astype(int)\n",
    "    TP = np.sum((pred_y == 1) & (test_y.values == 1))\n",
    "    TN = np.sum((pred_y == 0) & (test_y.values == 0))\n",
    "    FP = np.sum((pred_y == 1) & (test_y.values == 0))\n",
    "    FN = np.sum((pred_y == 0) & (test_y.values == 1))\n",
    "\n",
    "    acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "    prec_p = TP / (TP + FP) if (TP + FP) else np.nan\n",
    "    rec_p  = TP / (TP + FN) if (TP + FN) else np.nan\n",
    "    f1_p   = 2*prec_p*rec_p/(prec_p+rec_p) if (prec_p+rec_p) else np.nan\n",
    "\n",
    "    prec_n = TN / (TN + FN) if (TN + FN) else np.nan\n",
    "    rec_n  = TN / (TN + FP) if (TN + FP) else np.nan\n",
    "    f1_n   = 2*prec_n*rec_n/(prec_n+rec_n) if (prec_n+rec_n) else np.nan\n",
    "\n",
    "    rows.append([t, acc, prec_p, rec_p, f1_p, prec_n, rec_n, f1_n])\n",
    "\n",
    "score = pd.DataFrame(rows, columns=[\n",
    "    'Threshold', 'Accuracy', 'Precision P', 'Recall P', 'F1 score P',\n",
    "    'Precision N', 'Recall N', 'F1 score N'\n",
    "]).set_index('Threshold')\n",
    "```\n",
    "\n",
    "### C2) Plot\n",
    "\n",
    "```python\n",
    "score['Accuracy'].plot(grid=True).set_title('Accuracy vs threshold')\n",
    "```\n",
    "\n",
    "And a grid of the other metrics:\n",
    "\n",
    "```python\n",
    "fig, axs = plt.subplots(nrows=2, ncols=3, sharex=True, sharey=True, figsize=(10, 5))\n",
    "cols = ['Precision P', 'Recall P', 'F1 score P', 'Precision N', 'Recall N', 'F1 score N']\n",
    "for ax, col in zip(axs.flat, cols):\n",
    "    score[col].plot(ax=ax, grid=True)\n",
    "    ax.set_title(col)\n",
    "```\n",
    "\n",
    "Interpretation tip: increasing `t` usually **reduces** predicted positives → often **increases precision** and **decreases recall** for the positive class.\n",
    "\n",
    "---\n",
    "\n",
    "## Part D — Interpret coefficients (feature importance)\n",
    "\n",
    "For logistic regression, `coef_` contains one coefficient per feature. A larger positive coefficient increases the log-odds of class 1.\n",
    "\n",
    "```python\n",
    "coefs = pd.DataFrame({\n",
    "    'name': train_X_std.columns,\n",
    "    'value': logreg.coef_[0]\n",
    "}).sort_values('value')\n",
    "\n",
    "plt.figure(figsize=(6, 8))\n",
    "plt.barh(coefs['name'], coefs['value'], alpha=0.7)\n",
    "plt.title('Logistic regression coefficients')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Exam note: coefficients are comparable only if features are on comparable scales — that’s why standardization matters.\n",
    "\n",
    "---\n",
    "\n",
    "## Quiz answers (from `AppliedML_solutions.ipynb`)\n",
    "\n",
    "- **Question 1:** **a) F1 Score** (best single-number summary when classes are imbalanced and you care about both precision and recall).\n",
    "- **Question 2:** **d) True positive rate is 0.95** because `TPR = TP/(TP+FN) = 100/(100+5) ≈ 0.95`.\n",
    "\n",
    "---\n",
    "\n",
    "## Quick exam checklist (common pitfalls)\n",
    "\n",
    "- **Always align dummy columns:** one-hot encode train, then `reindex` test to training columns.\n",
    "- **Standardize using training stats only:** never compute mean/std on test.\n",
    "- **Know your confusion matrix convention:** write down which axis is actual vs predicted before computing metrics.\n",
    "- **Threshold moves precision/recall:** higher threshold → fewer predicted positives → typically higher precision, lower recall (positive class).\n",
    "- **Unbalanced classes:** accuracy can be misleading; prefer precision/recall/F1 and also inspect the confusion matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc35c7f4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
