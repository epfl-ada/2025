{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YDIB11/2025/blob/main/HW3_practical_ex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2VJsUBgJWVT"
      },
      "source": [
        "# Homework 3: optimization of a CNN model\n",
        "The task of this homework is to optimize a CNN model for the CIFAR-100. You are free to define the architecture of the model, and the training procedure. The only contraints are:\n",
        "- It must be a `torch.nn.Module` object\n",
        "- The number of trained parameters must be less than 1 million\n",
        "- The test dataset must not be used for any step of training.\n",
        "- The final training notebook should run on Google Colab within a maximum 1 hour approximately.\n",
        "- Do not modify the random seed, as they are needed for reproducibility purpose.\n",
        "\n",
        "For the grading, you must use the `evaluate` function defined below. It takes a model as input, and returns the test accuracy as output.\n",
        "\n",
        "As a guideline, you are expected to **discuss** and motivate your choices regarding:\n",
        "- Model architecture\n",
        "- Hyperparameters (learning rate, batch size, etc)\n",
        "- Regularization methods\n",
        "- Optimizer\n",
        "- Validation scheme\n",
        "\n",
        "A code without any explanation of the choices will not be accepted. Test accuracy is not the only measure of success for this homework.\n",
        "\n",
        "Remember that most of the train process is randomized, store your model's weights after training and load it before the evaluation!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "t0 = time.time()"
      ],
      "metadata": {
        "id": "GH6yhr_yU6iT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW0WZTgcJWVU"
      },
      "source": [
        "## Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOUqpef_JWVU"
      },
      "source": [
        "### Loading packages and libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8T3zV7aSJWVU",
        "outputId": "c0ea8dd6-2397-46ff-a8b9-ffea43b02be2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "\n",
        "# Fix all random seeds\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# For full determinism\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Import the best device available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "# load the data\n",
        "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fBvtGTzJWVV"
      },
      "outputs": [],
      "source": [
        "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
        "\n",
        "def evaluate(model):\n",
        "    params_count = sum(p.numel() for p in model.parameters())\n",
        "    print('The model has {} parameters'.format(params_count))\n",
        "\n",
        "    if params_count > int(1e6):\n",
        "        print('The model has too many parameters! Not allowed to evaluate.')\n",
        "        return\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "\n",
        "    # print in bold red in a notebook\n",
        "    print('\\033[1m\\033[91mAccuracy on the test set: {}%\\033[0m'.format(100 * correct / total))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDQAuvCQJWVV"
      },
      "source": [
        "### Example of a simple CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IyNGmNlJWVV",
        "outputId": "1e7d63bd-a538-4a5e-d1e9-327478e474d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters:  556708\n"
          ]
        }
      ],
      "source": [
        "class TinyNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TinyNet, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = torch.nn.Linear(8*8*64, 128)\n",
        "        self.fc2 = torch.nn.Linear(128, 100)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.relu(self.conv1(x))\n",
        "        x = torch.nn.functional.max_pool2d(x, 2)\n",
        "        x = torch.nn.functional.relu(self.conv2(x))\n",
        "        x = torch.nn.functional.max_pool2d(x, 2)\n",
        "        x = x.view(-1, 8*8*64)\n",
        "        x = torch.nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "print(\"Model parameters: \", sum(p.numel() for p in TinyNet().parameters()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46vQG1f2JWVW"
      },
      "source": [
        "### Example of basic training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4BCysK9JWVW",
        "outputId": "b60b7380-4efa-4bc9-867c-36006ed72dd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 4.6138\n",
            "Epoch [2/10], Loss: 4.5809\n",
            "Epoch [3/10], Loss: 4.5756\n",
            "Epoch [4/10], Loss: 4.5898\n",
            "Epoch [5/10], Loss: 4.5962\n",
            "Epoch [6/10], Loss: 4.5979\n",
            "Epoch [7/10], Loss: 4.6147\n",
            "Epoch [8/10], Loss: 4.5324\n",
            "Epoch [9/10], Loss: 4.5006\n",
            "Epoch [10/10], Loss: 4.3964\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model = TinyNet()\n",
        "model.to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "for epoch in range(10):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 10, loss.item()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APXHOgAIJWVW",
        "outputId": "eb72668a-685c-4601-f5c9-7d6a9d7aab84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 556708 parameters\n",
            "\u001b[1m\u001b[91mAccuracy on the test set: 3.36%\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# save the model on a file\n",
        "torch.save(model.state_dict(), 'tiny_net.pt')\n",
        "\n",
        "loaded_model = TinyNet()\n",
        "loaded_model.load_state_dict(torch.load('tiny_net.pt', weights_only=True))\n",
        "evaluate(loaded_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Solution**\n",
        "\n",
        "## Setup choices\n",
        "- Use torch/torchvision with AMP (`torch.amp.autocast`, `GradScaler`) for faster training on GPU.\n",
        "- Cosine LR scheduler for smooth decay without manual milestones.\n",
        "- We reuse earlier seed/device setup from the example cells.\n"
      ],
      "metadata": {
        "id": "PoRC7lTwKJNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import autocast, GradScaler # Corrected from GradScalerQ\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import torch.nn.functional as F\n",
        "from torch.amp import autocast, GradScaler"
      ],
      "metadata": {
        "id": "NMWMvbWeKKUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data\n",
        "mean = (0.5071, 0.4867, 0.4408)\n",
        "std = (0.2675, 0.2565, 0.2761)\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomErasing(p=0.25)\n",
        "])\n",
        "\n",
        "val_tf = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "full_train = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=train_tf)\n",
        "val_size = 5000\n",
        "train_size = len(full_train) - val_size\n",
        "train_set, val_indices = random_split(full_train, [train_size, val_size])\n",
        "# Rebuild val subset with val transforms\n",
        "val_set = Subset(torchvision.datasets.CIFAR100(root='./data', train=True, download=False, transform=val_tf),\n",
        "                 val_indices.indices)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_set, batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# Model (depthwise-separable residual blocks)\n",
        "class DWBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, stride=1):\n",
        "        super().__init__()\n",
        "        mid = out_ch // 2\n",
        "        self.reduce = nn.Conv2d(in_ch, mid, 1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(mid)\n",
        "        self.dw = nn.Conv2d(mid, mid, 3, stride=stride, padding=1, groups=mid, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(mid)\n",
        "        self.expand = nn.Conv2d(mid, out_ch, 1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_ch)\n",
        "        self.skip = nn.Conv2d(in_ch, out_ch, 1, stride=stride, bias=False) if (in_ch != out_ch or stride != 1) else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = F.relu(self.bn1(self.reduce(x)))\n",
        "        out = F.relu(self.bn2(self.dw(out)))\n",
        "        out = self.bn3(self.expand(out))\n",
        "        if self.skip is not None:\n",
        "            identity = self.skip(identity)\n",
        "        return F.relu(out + identity)\n",
        "\n",
        "class SmallNet(nn.Module):\n",
        "    def __init__(self, num_classes=100):\n",
        "        super().__init__()\n",
        "        mean = torch.tensor([0.5071, 0.4867, 0.4408]).view(1,3,1,1)\n",
        "        std = torch.tensor([0.2675, 0.2565, 0.2761]).view(1,3,1,1)\n",
        "        self.register_buffer('mean', mean)\n",
        "        self.register_buffer('std', std)\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(3, 96, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.stage1 = nn.Sequential(DWBlock(96, 128), DWBlock(128, 128))\n",
        "        self.stage2 = nn.Sequential(DWBlock(128, 192, stride=2), DWBlock(192, 192))\n",
        "        self.stage3 = nn.Sequential(DWBlock(192, 256, stride=2), DWBlock(256, 256))\n",
        "        self.stage4 = nn.Sequential(DWBlock(256, 448, stride=2), DWBlock(448, 448))\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(448, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "          x = (x - self.mean) / self.std\n",
        "          x = self.stem(x)\n",
        "          x = self.stage1(x)\n",
        "          x = self.stage2(x)\n",
        "          x = self.stage3(x)\n",
        "          x = self.stage4(x)\n",
        "          x = self.pool(x).flatten(1)\n",
        "          return self.fc(x)\n",
        "\n",
        "model = SmallNet().to(device)\n",
        "print(\"Trainable params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvCkz_dtKQrY",
        "outputId": "53fb5179-f8a6-4a83-d080-089ef9698e7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable params: 845380\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data & model design\n",
        "- Augmentations: random crop + flip + random erasing to improve generalization on CIFAR-100; no normalization in transforms because we normalize inside the model, so train/val/test all see consistent preprocessing.\n",
        "- Validation split: 5k held-out from training via `random_split`; val loader uses only ToTensor to match test-time preprocessing.\n",
        "- Model (SmallNet): depthwise-separable residual blocks keep params low (<1M) while retaining representational power; residual skips help optimization. Channel progression 96→128→192→256→448 gives ~0.84M params, under the 1M cap.\n",
        "- In-model normalization: registered mean/std buffers apply CIFAR-100 normalization in `forward`, ensuring consistency with the provided `evaluate` loader (which uses raw ToTensor).\n"
      ],
      "metadata": {
        "id": "ZlQBiXxrTlT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train/val loop\n",
        "epochs = 50\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "scaler = GradScaler('cuda')\n",
        "best_val = 0.0\n",
        "best_path = 'best_smallnet.pth'\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    tot_loss = tot_correct = tot_seen = 0\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad()\n",
        "        with autocast('cuda'):\n",
        "          logits = model(imgs)\n",
        "          loss = criterion(logits, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        tot_loss += loss.item() * labels.size(0)\n",
        "        tot_correct += (logits.argmax(1) == labels).sum().item()\n",
        "        tot_seen += labels.size(0)\n",
        "\n",
        "    scheduler.step()\n",
        "    train_loss = tot_loss / tot_seen\n",
        "    train_acc = tot_correct / tot_seen\n",
        "\n",
        "    model.eval()\n",
        "    val_correct = val_seen = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "            logits = model(imgs)\n",
        "            val_correct += (logits.argmax(1) == labels).sum().item()\n",
        "            val_seen += labels.size(0)\n",
        "    val_acc = val_correct / val_seen\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - train_loss {train_loss:.4f} - train_acc {train_acc:.4f} - val_acc {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_val:\n",
        "        best_val = val_acc\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "\n",
        "print(\"Best val_acc:\", best_val)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yt4e5fyKTpX",
        "outputId": "9ed9dd48-fa61-45de-c3fe-b0c88cbd1cf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50 - train_loss 4.1262 - train_acc 0.0838 - val_acc 0.1578\n",
            "Epoch 2/50 - train_loss 3.4942 - train_acc 0.2039 - val_acc 0.2264\n",
            "Epoch 3/50 - train_loss 3.1045 - train_acc 0.2945 - val_acc 0.2660\n",
            "Epoch 4/50 - train_loss 2.8715 - train_acc 0.3593 - val_acc 0.3620\n",
            "Epoch 5/50 - train_loss 2.7240 - train_acc 0.3983 - val_acc 0.4066\n",
            "Epoch 6/50 - train_loss 2.6113 - train_acc 0.4320 - val_acc 0.4196\n",
            "Epoch 7/50 - train_loss 2.5299 - train_acc 0.4524 - val_acc 0.4056\n",
            "Epoch 8/50 - train_loss 2.4701 - train_acc 0.4710 - val_acc 0.4532\n",
            "Epoch 9/50 - train_loss 2.4201 - train_acc 0.4876 - val_acc 0.3614\n",
            "Epoch 10/50 - train_loss 2.3780 - train_acc 0.5015 - val_acc 0.4330\n",
            "Epoch 11/50 - train_loss 2.3344 - train_acc 0.5140 - val_acc 0.4708\n",
            "Epoch 12/50 - train_loss 2.3030 - train_acc 0.5208 - val_acc 0.4914\n",
            "Epoch 13/50 - train_loss 2.2716 - train_acc 0.5334 - val_acc 0.4718\n",
            "Epoch 14/50 - train_loss 2.2405 - train_acc 0.5409 - val_acc 0.4982\n",
            "Epoch 15/50 - train_loss 2.2097 - train_acc 0.5541 - val_acc 0.4954\n",
            "Epoch 16/50 - train_loss 2.1875 - train_acc 0.5588 - val_acc 0.5134\n",
            "Epoch 17/50 - train_loss 2.1617 - train_acc 0.5695 - val_acc 0.5504\n",
            "Epoch 18/50 - train_loss 2.1442 - train_acc 0.5716 - val_acc 0.5192\n",
            "Epoch 19/50 - train_loss 2.1140 - train_acc 0.5817 - val_acc 0.5246\n",
            "Epoch 20/50 - train_loss 2.0867 - train_acc 0.5933 - val_acc 0.5278\n",
            "Epoch 21/50 - train_loss 2.0611 - train_acc 0.6006 - val_acc 0.5500\n",
            "Epoch 22/50 - train_loss 2.0362 - train_acc 0.6086 - val_acc 0.5662\n",
            "Epoch 23/50 - train_loss 2.0061 - train_acc 0.6203 - val_acc 0.5646\n",
            "Epoch 24/50 - train_loss 1.9754 - train_acc 0.6278 - val_acc 0.5878\n",
            "Epoch 25/50 - train_loss 1.9554 - train_acc 0.6354 - val_acc 0.5658\n",
            "Epoch 26/50 - train_loss 1.9318 - train_acc 0.6424 - val_acc 0.5880\n",
            "Epoch 27/50 - train_loss 1.8933 - train_acc 0.6563 - val_acc 0.5960\n",
            "Epoch 28/50 - train_loss 1.8709 - train_acc 0.6637 - val_acc 0.6062\n",
            "Epoch 29/50 - train_loss 1.8458 - train_acc 0.6744 - val_acc 0.6076\n",
            "Epoch 30/50 - train_loss 1.8184 - train_acc 0.6837 - val_acc 0.6404\n",
            "Epoch 31/50 - train_loss 1.7903 - train_acc 0.6933 - val_acc 0.6278\n",
            "Epoch 32/50 - train_loss 1.7572 - train_acc 0.7038 - val_acc 0.6332\n",
            "Epoch 33/50 - train_loss 1.7270 - train_acc 0.7145 - val_acc 0.6438\n",
            "Epoch 34/50 - train_loss 1.6930 - train_acc 0.7254 - val_acc 0.6530\n",
            "Epoch 35/50 - train_loss 1.6629 - train_acc 0.7349 - val_acc 0.6634\n",
            "Epoch 36/50 - train_loss 1.6356 - train_acc 0.7449 - val_acc 0.6672\n",
            "Epoch 37/50 - train_loss 1.6013 - train_acc 0.7589 - val_acc 0.6764\n",
            "Epoch 38/50 - train_loss 1.5710 - train_acc 0.7692 - val_acc 0.6888\n",
            "Epoch 39/50 - train_loss 1.5374 - train_acc 0.7833 - val_acc 0.6924\n",
            "Epoch 40/50 - train_loss 1.5093 - train_acc 0.7907 - val_acc 0.6942\n",
            "Epoch 41/50 - train_loss 1.4798 - train_acc 0.8042 - val_acc 0.6988\n",
            "Epoch 42/50 - train_loss 1.4522 - train_acc 0.8137 - val_acc 0.6972\n",
            "Epoch 43/50 - train_loss 1.4291 - train_acc 0.8222 - val_acc 0.7038\n",
            "Epoch 44/50 - train_loss 1.4127 - train_acc 0.8296 - val_acc 0.7094\n",
            "Epoch 45/50 - train_loss 1.3888 - train_acc 0.8401 - val_acc 0.7144\n",
            "Epoch 46/50 - train_loss 1.3731 - train_acc 0.8458 - val_acc 0.7132\n",
            "Epoch 47/50 - train_loss 1.3600 - train_acc 0.8508 - val_acc 0.7162\n",
            "Epoch 48/50 - train_loss 1.3499 - train_acc 0.8544 - val_acc 0.7184\n",
            "Epoch 49/50 - train_loss 1.3429 - train_acc 0.8575 - val_acc 0.7200\n",
            "Epoch 50/50 - train_loss 1.3435 - train_acc 0.8590 - val_acc 0.7182\n",
            "Best val_acc: 0.72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training setup\n",
        "- Optimizer: SGD with momentum 0.9 and weight decay 5e-4.\n",
        "- LR: 0.1 with cosine annealing over 50 epochs.\n",
        "- Loss: CrossEntropy with label smoothing 0.1 for regularization.\n",
        "- AMP: mixed precision (`autocast`, `GradScaler`) plus grad clipping for stability and speed.\n",
        "- Batch sizes: 128 train / 256 val to balance throughput and memory.\n",
        "- Checkpointing: save best model by validation accuracy.\n"
      ],
      "metadata": {
        "id": "1nO8tH7sT4Ix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best and run provided evaluate() on the test set\n",
        "model.load_state_dict(torch.load(best_path, map_location=device))\n",
        "model.to(device)\n",
        "evaluate(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBPr8cG3KWGH",
        "outputId": "54aae61f-8bd1-418a-d528-66e9cfb1fd5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 845380 parameters\n",
            "\u001b[1m\u001b[91mAccuracy on the test set: 72.06%\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation\n",
        "- Reload best checkpoint (highest val acc) before test.\n",
        "- Use the provided `evaluate` to validate.\n"
      ],
      "metadata": {
        "id": "kfYsXAOOUDs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = time.time()\n",
        "elapsed = t1 - t0\n",
        "print(f\"Total wall time: {elapsed/60:.1f} min\")\n",
        "if elapsed < 3600:\n",
        "    print(\"✅ End-to-end run completed in under 1 hour.\")\n",
        "else:\n",
        "    print(\"⚠️ Took longer than 1 hour.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LBKqPMWU_QF",
        "outputId": "93a46872-99e6-49bc-e439-c6d0febbfca2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total wall time: 19.6 min\n",
            "✅ End-to-end run completed in under 1 hour.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}